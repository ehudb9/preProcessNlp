{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pre-process workshop.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ehudb9/preProcessNlp/blob/main/Pre_process_workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6odhHwejCOEq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a23bd530-8eba-4b47-aa84-5d6b7bc5899b"
      },
      "source": [
        "!pip install emoji \n",
        "!git clone https://github.com/aub-mind/arabert.git\n",
        "!git clone https://github.com/MagedSaeed/farasapy.git\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (0.6.0)\n",
            "fatal: destination path 'arabert' already exists and is not an empty directory.\n",
            "fatal: destination path 'farasapy' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvy2ZfWnRz9J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a70d0ce-5f8c-4e4b-bc4c-b17ba007babe"
      },
      "source": [
        "# to count words in string \n",
        "# using regex (findall()) \n",
        "#<Avrahami>: What is this code used for? why do we need it? - basic stats\n",
        "# This function will help us count number of words in a sentence.\n",
        "# Sentence with up to 5 words will be irrelevent for our algorithm.\n",
        "#why? short sentences justisying/ Deny the authors twit and don't share new info.\n",
        "import re \n",
        "\n",
        "# initializing string \n",
        "test_string = \"من أين أنت\"\n",
        "\n",
        "# printing original string \n",
        "print (\"The original string is : \" + test_string) \n",
        "\n",
        "# using regex (findall()) \n",
        "# to count words in string \n",
        "res = len(re.findall(r'\\w+', test_string)) \n",
        "\n",
        "# printing result \n",
        "print (\"The number of words in string are : \" + str(res)) \n",
        "\n",
        "def words_counter(text):\n",
        "  res = len(re.findall(r'\\w+', test_string))\n",
        "  return res\n",
        "\n",
        "test_string = \"من أين أنت\"\n",
        "words_counter(test_string)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The original string is : من أين أنت\n",
            "The number of words in string are : 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGGjg1V0ze0o",
        "outputId": "c608089b-e626-49d3-c39c-f4c6261519c9"
      },
      "source": [
        "# Non-Arabic characters removal- keep arabic and numeric chars and remove the rest\r\n",
        "#!/usr/bin/env python\r\n",
        "import re\r\n",
        "\r\n",
        "text =  u' من أين أنت  '\r\n",
        "\r\n",
        "def Remove_english(text):\r\n",
        "\r\n",
        "  t = re.sub(r'[^0-9\\u0600-\\u06ff\\u0750-\\u077f\\ufb50-\\ufbc1\\ufbd3-\\ufd3f\\ufd50-\\ufd8f\\ufd50-\\ufd8f\\ufe70-\\ufefc\\uFDF0-\\uFDFD]+', ' ', text)\r\n",
        "  return t\r\n",
        "\r\n",
        "print(Remove_english(text))\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " من أين أنت \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcMw17WDT2yg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a405c8bb-df81-4561-f992-820d8e8d7fc5"
      },
      "source": [
        "#<Avrahami>: What is this code used for? why do we need it? - not really required\n",
        "# Emoji removal\n",
        "#!/usr/bin/env python\n",
        "import re\n",
        "\n",
        "text = u'\\U0001f602 من أين أنت '\n",
        "print(text) # with emoji\n",
        "\n",
        "def deEmojify(text):\n",
        "    regrex_pattern = re.compile(pattern = \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags = re.UNICODE)\n",
        "    return regrex_pattern.sub(r'',text)\n",
        "\n",
        "print(deEmojify(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "😂 من أين أنت \n",
            " من أين أنت \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdmickCrVO0M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4b3de57-fd6f-4373-ce27-f6fe5473b67b"
      },
      "source": [
        "#<Avrahami>: What is this code used for? why do we need it? - emoji counter\n",
        "#emoji counter + word counter \n",
        "import emoji\n",
        "import regex\n",
        "\n",
        "arabic= \"حسبي الله عليهم💔\"\n",
        "line = \"hello 👩🏾‍🎓 emoji hello 👨‍👩‍👦‍👦 how are 😊 you today🙅🏽🙅🏽\"\n",
        "counter1 = split_count(arabic)\n",
        "counter2 = split_count(line)\n",
        "print(\"Number of emojis arabic - {}, number of words - {}\".format(counter1[0], counter1[1]))\n",
        "print(\"Number of emojis line - {}, number of words - {}\".format(counter2[0], counter2[1]))\n",
        "def split_count(text):\n",
        "    emoji_counter = 0\n",
        "    data = regex.findall(r'\\X', text)\n",
        "    for word in data:\n",
        "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
        "            emoji_counter += 1\n",
        "            # Remove from the given text the emojis\n",
        "            text = text.replace(word, '') \n",
        "\n",
        "    words_counter = len(text.split())\n",
        "\n",
        "    return emoji_counter, words_counter"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of emojis arabic - 1, number of words - 3\n",
            "Number of emojis line - 5, number of words - 7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-kLBm4GYP2u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2521bcb6-f6cf-4ad2-bcf7-3ef9f2e24ec4"
      },
      "source": [
        "#<Avrahami>: \n",
        "#   (i) I have renamed the function\n",
        "#   (ii) add an input checker - make sure we indeed got a string\n",
        "#   (iii) you are currntly removing ALL chars which are consecutive. What about valid words with two (or more) consecutive chars (e.g., in English words like bool, meeting)\n",
        "#   (iv) seems like you assume all chars are arabic chars. What about punctuations? what about emojies? I would say it is better first to tokenize the text and then apply the logic\n",
        "#   (vi) add documantation - parameters explanation + what the function is doing \n",
        "#Erase duplicates in a word \n",
        "def consecutive_chars_removal(string):\n",
        "    list = []\n",
        "    prev = \"\"\n",
        "    count = 0 \n",
        "    for ch in string:\n",
        "        if ch != prev:\n",
        "            list.append(ch)\n",
        "        \"\"\"\n",
        "        # a special case of two specifc letters in arabic\n",
        "        elif ch == \"ل\" or ch == \"ه\":\n",
        "          if count < 2 :\n",
        "             count +=1\n",
        "             list.append(ch)\n",
        "          else:\n",
        "            count = 0 \n",
        "        \"\"\"\n",
        "        prev = ch  \n",
        "    return ''.join(list)        \n",
        "\n",
        "string = \"شوفتكككككككككككككككككككككك\"\n",
        "string1=\"تههههههههههن\"\n",
        "string2=\"عظييييييممممممم\"\n",
        "string3=\"لاانلنتتىلللللللل الة الا اللق\"\n",
        "print(fix(string))\n",
        "print(fix(string1))\n",
        "print(fix(string2))\n",
        "print(fix(string3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "شوفتك\n",
            "تهن\n",
            "عظيم\n",
            "لانلنتىل الة الا الق\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xi8ix3ibZTFE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27bad56d-74a6-4bd1-fa98-ff93748c455f"
      },
      "source": [
        "#<Avrahami>: What is this code used for? why do we need it?\n",
        "#Emoji translate\n",
        "from emoji.unicode_codes import UNICODE_EMOJI\n",
        "\n",
        "emoji = '😊'\n",
        "\n",
        "print(UNICODE_EMOJI[emoji])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ":smiling_face_with_smiling_eyes:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOFZJy0lkscS"
      },
      "source": [
        "\n",
        "#<Avrahami>: What is this code used for? why do we need it?\n",
        "#https://pypi.org/project/emosent-py/\n",
        "#https://github.com/FintelLabs/emosent-py/tree/master/emosent\n",
        "#http://kt.ijs.si/data/Emoji_sentiment_ranking/\n",
        "#http://kt.ijs.si/data/Emoji_sentiment_ranking/emojimap.html\n",
        "!pip install emosent-py\n",
        "from emosent import get_emoji_sentiment_rank\n",
        "get_emoji_sentiment_rank('💔')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2LeACkGmq4b"
      },
      "source": [
        "!pip install emosent-py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kD7i-YPP9kXX"
      },
      "source": [
        "!pip install git+https://github.com/aub-mind/arabert.git\n",
        "from cltk.corpus.utils.importer import CorpusImporter\n",
        " c = CorpusImporter('arabic')\n",
        " c.list_corpora\n",
        " c.import_corpus('arabic_text_perseus') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whe0XNwrxtcA"
      },
      "source": [
        "!pip install git+https://github.com/CAMeL-Lab/camel_tools.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFj6bl-vnoXX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6_x01CfsA9h"
      },
      "source": [
        "tokenizer_obj = arabic_tokenizer(model_name='arabert')\n",
        "text_to_tokenize = 'قوانينها و مدة اللعب اطول من حياتي وكأنها لعبة من العصر الهيروغليفي، ما فهتمتها و بعرفش كيف ربحت لما لعبتها مرة بس.'\n",
        "tokenizer_obj.tokenize(text_to_tokenize, remove_punckt=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOwvjC5-pueM",
        "outputId": "e1e06e7c-a1e2-457e-e769-b9903f8801bd"
      },
      "source": [
        "import string\r\n",
        "known_punct = string.punctuation\r\n",
        "my_tokenized_data = ['asa!!~@#', '##', 'aaa']\r\n",
        "\r\n",
        "tokenized_text_filtered = [t for t in my_tokenized_data if not all(l in string.punctuation for l in t)]\r\n",
        "tokenized_text_filtered"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['asa!!~@#', 'aaa']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oBMnjhHaaaF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JG06pXfmZ66W"
      },
      "source": [
        "##**lematizetion**\r\n",
        " list for checking:\r\n",
        "\r\n",
        "\r\n",
        "1.   X:farasa - --need to find api\r\n",
        "2.   qalsadi \r\n",
        "3.  X: ALKHALIL ---need to find api\r\n",
        "4. nltk ==> returns the root. us ut good?\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-jDM_qbbEMc"
      },
      "source": [
        "!pip install qalsadi\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8MxJWaMaUaK"
      },
      "source": [
        "# lematization --- returns the root ---> to check with avrahami\r\n",
        "\r\n",
        "\r\n",
        "from nltk.stem.isri import ISRIStemmer\r\n",
        "st = ISRIStemmer()\r\n",
        "print(st.stem(u''))\r\n",
        "\r\n",
        "#print(ISRIStemmer().suf32(\"اعلاميون\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fbzwMB-d0ef"
      },
      "source": [
        "# using qalsadi  --> more options in https://pypi.org/project/qalsadi/  + analysing\r\n",
        "\r\n",
        "import qalsadi.lemmatizer\r\n",
        "text = u\"\"\"هل تحتاج إلى ترجمة كي تفهم خطاب الملك؟ اللغة \"الكلاسيكية\" (الفصحى) موجودة في كل اللغات وكذلك اللغة \"الدارجة\" .. الفرنسية التي ندرس في المدرسة ليست الفرنسية التي يستخدمها الناس في شوارع باريس .. وملكة بريطانيا لا تخطب بلغة شوارع لندن .. لكل مقام مقال\"\"\"\r\n",
        "lemmer = qalsadi.lemmatizer.Lemmatizer()\r\n",
        "# lemmatize a word\r\n",
        "print(lemmer.lemmatize(\"يحتاج\"))\r\n",
        "\r\n",
        "# lemmatize a word with a specific pos\r\n",
        "print(lemmer.lemmatize(\"وفي\"))\r\n",
        "lemmer.lemmatize(\"وفي\", pos=\"v\")\r\n",
        "\r\n",
        "# lemmatize a text to list\r\n",
        "lemmas = lemmer.lemmatize_text(text)\r\n",
        "print(lemmas)\r\n",
        "\r\n",
        "# lemmatize a text and return lemma pos\r\n",
        "lemmas = lemmer.lemmatize_text(text, return_pos=True)\r\n",
        "print(lemmas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnZCFJ_xplvw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rz2Rr1GJplyz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vfNIV-ooyn5"
      },
      "source": [
        "# **pre-process class**\r\n",
        "last update: 21.12.2020, Avrahami"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hL9vxsW0o5Ww"
      },
      "source": [
        "#installations and imports\r\n",
        "!pip install qalsadi\r\n",
        "! pip install transformers\r\n",
        "! pip install pyarabic\r\n",
        "!pip install emoji\r\n",
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "\r\n",
        "from transformers import AutoTokenizer, BertTokenizer\r\n",
        "import pyarabic.araby as araby\r\n",
        "from emoji.unicode_codes import UNICODE_EMOJI\r\n",
        "import string\r\n",
        "\r\n",
        "import re \r\n",
        "import qalsadi.lemmatizer\r\n",
        "from nltk.stem.isri import ISRIStemmer\r\n",
        "from emoji.unicode_codes import UNICODE_EMOJI\r\n",
        "#from emosent import get_emoji_sentiment_rank\r\n",
        "import regex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rarmuzonogt"
      },
      "source": [
        "class arabic_tokenizer(object):\n",
        "    \"\"\" Class for tokenize arabic text in different methods\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "\n",
        "        Attributes\n",
        "        ----------\n",
        "\n",
        "        Examples\n",
        "        --------\n",
        "\n",
        "        \"\"\"\n",
        "    def __init__(self, model_name):\n",
        "      self.model_name = model_name\n",
        "      if self.model_name == 'arabert':\n",
        "        model_str = 'aubmindlab/bert-base-arabert'\n",
        "        arabert_tokenizer = AutoTokenizer.from_pretrained(model_str)\n",
        "        arabert_tokenizer = self.add_emojies_to_tokenizer(arabert_tokenizer)\n",
        "        self.tokenizer = arabert_tokenizer.tokenize\n",
        "      elif self.model_name == 'gigabert':\n",
        "        model_str = 'lanwuwei/GigaBERT-v4-Arabic-and-English'\n",
        "        gigabert_tokenizer = BertTokenizer.from_pretrained(model_str, do_lower_case=True)\n",
        "        gigabert_tokenizer = self.add_emojies_to_tokenizer(gigabert_tokenizer)\n",
        "        self.tokenizer = gigabert_tokenizer.tokenize\n",
        "      elif self.model_name == 'nltk':\n",
        "        self.tokenizer = nltk.word_tokenize\n",
        "      elif self.model_name == 'pyarabic':\n",
        "        self.tokenizer = araby.tokenize\n",
        "    def get_model(self):\n",
        "      return self.model\n",
        "\n",
        "    def set_model(self, model_name):\n",
        "      self.model = model_name\n",
        "      \n",
        "    def tokenize(self, text, remove_punckt=True):\n",
        "      tokenized_text = self.tokenizer(text)\n",
        "      if remove_punckt:\n",
        "        known_punct = string.punctuation\n",
        "        tokenized_text_filtered = [t for t in tokenized_text if not all(l in string.punctuation for l in t)]\n",
        "        return tokenized_text_filtered\n",
        "      else:\n",
        "        return tokenized_text\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def add_emojies_to_tokenizer(tokenizer):\n",
        "      for cur_emoji in UNICODE_EMOJI.keys():\n",
        "        tokenizer.add_tokens(cur_emoji)\n",
        "      return tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dgqgh9-CiBF1"
      },
      "source": [
        "class preProcess:\r\n",
        "  \"\"\" Class for pre-processing text for analyzing\r\n",
        "      gets a text and creating a preProcess object with the following Attributes:\r\n",
        "      1. text :  the input text\r\n",
        "      2. Emojie : the text with no emojies , emojies sentiment\r\n",
        "      3. text satistic: word counts , sentences\r\n",
        "      4. cleaner : the text after Removing punctuation, stop words and emojies, and Merging duplicates ( ignoring the letter \"L\" - for allah) \r\n",
        "      5.Tokenize : list of the words after cleaner and tokenization \r\n",
        "      6. Lemmatization : list of the words after cleaner and tokenization and Lemmatization\r\n",
        "\r\n",
        "      pipline of the process : text input(string) :\r\n",
        "      *Emoji sentiment and remove emojies\r\n",
        "      *text satistic:word counts,sentences\r\n",
        "      *cleaner: \r\n",
        "          1. Removing punctuation\r\n",
        "          2. Removing stop words and emojies\r\n",
        "          3. Merging duplicates ( ignoring the letter \"L\" - for allah)\r\n",
        "      *Tokenizer the text\r\n",
        "      *Lemmatization\r\n",
        "        Parameters\r\n",
        "        :text: string for processing\r\n",
        "\r\n",
        "        Examples\r\n",
        "        text =\"شركة شراء اثاث مستعمل بالرياض بافضل الاسعار \"\r\n",
        "        print (test.lemmatization) --> ['شركة', 'شراء', 'اثاث', 'مستعمل', 'رياض', 'بافضل', 'الاسعار']\r\n",
        "\r\n",
        "        \"\"\"\r\n",
        "  def __init__(self, tokenizer_model='gigabert', remove_consecutive_chars=True, pure_arabic=True,punc_remove=True,remove_emoji=True, extract_emoji_sentiment=False):\r\n",
        "    \"\"\"   \r\n",
        "    creating a preProcess object with the following Attributes:\r\n",
        "      1. text :  the input text\r\n",
        "      2. Emojie : the text with no emojies , emojies sentiment\r\n",
        "      3. text satistic: word counts , sentences\r\n",
        "      4. cleaner : text after Removing punctuation, stop words and emojies, and Merging duplicates ( ignoring the letter \"L\" - for allah) \r\n",
        "      5. Tokenizer : list of the words after cleaner and tokenization \r\n",
        "      6. Lemmatization : list of the words after cleaner and tokenization and Lemmatization\r\n",
        "  \r\n",
        "    Parameters: \r\n",
        "    :text (string): string for processing\r\n",
        "   \r\n",
        "    \"\"\"\r\n",
        "    self.remove_consecutive_chars = remove_consecutive_chars\r\n",
        "    self.punc_remove = punc_remove\r\n",
        "    self.extract_emoji_sentiment = extract_emoji_sentiment\r\n",
        "    self.remove_emoji = remove_emoji\r\n",
        "    self.pure_arabic = pure_arabic\r\n",
        "    self.tokenizer = arabic_tokenizer(model_name=tokenizer_model)\r\n",
        "    self.satistics = None\r\n",
        "    self.emojies_sentiment = None\r\n",
        "    self.tokenized_text = None\r\n",
        "    self.lemmatized_text = None\r\n",
        "     \r\n",
        "  def transform(self, text):\r\n",
        "    #checking if the text is string  \r\n",
        "    if (not isinstance(text, str)) : \r\n",
        "      raise Exception(\"The input is not a string\")\r\n",
        "    new_text = text\r\n",
        "    if self.pure_arabic:\r\n",
        "      new_text = self.remove_non_arabic_char(new_text)\r\n",
        "    if self.remove_consecutive_chars:\r\n",
        "      new_text = self.consecutive_chars_removal(new_text)\r\n",
        "    if self.punc_remove:\r\n",
        "      new_text = self.punc_removal(new_text)\r\n",
        "    if self.extract_emoji_sentiment:\r\n",
        "      self.emojies_sentiment =  self.emoji_sentiment()    #to implemete tne sentimente\r\n",
        "    if self.remove_emoji:\r\n",
        "      new_text = self.deEmojify(new_text)\r\n",
        "    \r\n",
        "    self.satistics = self.words_counter(new_text) # int: num of words\r\n",
        "    self.tokenized_text = self.tokenizer.tokenize(new_text, remove_punckt=self.punc_remove)\r\n",
        "    self.lemmatized_text = self.lemmatizer(new_text)\r\n",
        "    return new_text\r\n",
        "  \r\n",
        "  \r\n",
        "  \"\"\" to add more satistic parameters\r\n",
        "  def calc_stats(text):\r\n",
        "    #checking if the text is string  \r\n",
        "    if (not isinstance(text, str)) : \r\n",
        "      raise Exception(\"The input is not a string\")\r\n",
        "    self.satistics = {\"word_count\" : self.words_counter() } # te comlete sentence counter\r\n",
        "    return self.satistics\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  # emoji func\r\n",
        "  def emoji_sentiment(self):\r\n",
        "    # lack of implementation!!! neet to check if needed with avrahami and if to do it before or after cleanning\r\n",
        "    return \"in process\"\r\n",
        "  \r\n",
        "  # satistic func\r\n",
        "  def words_counter(self, text):\r\n",
        "    \"\"\"\r\n",
        "    gets string and returns th number of the word (int) \r\n",
        "  \r\n",
        "    \"\"\"  \r\n",
        "    return len(re.findall(r'\\w+', text))\r\n",
        "  \r\n",
        "  # Non-Arabic characters removal- keep arabic and numeric chars and remove the rest\r\n",
        "  def remove_non_arabic_char(self, text):\r\n",
        "  # Gets String and remove non arabic letters\r\n",
        "    t = re.sub(r'[^0-9\\u0600-\\u06ff\\u0750-\\u077f\\ufb50-\\ufbc1\\ufbd3-\\ufd3f\\ufd50-\\ufd8f\\ufd50-\\ufd8f\\ufe70-\\ufefc\\uFDF0-\\uFDFD-\\~@#$^*()_+=[\\]{}|\\\\,.?: -@#$% <>` \"]+', ' ', text)\r\n",
        "    return t\r\n",
        "\r\n",
        "\r\n",
        "  #cleaner\r\n",
        "  def punc_removal(self, text):\r\n",
        "    \"\"\"\r\n",
        "  gets string and returns the string  after Removing punctuation, stop words and emojies,\r\n",
        "  and Merging duplicates ( ignoring the letter \"L\" - for allah)\r\n",
        "  \r\n",
        "    \"\"\"  \r\n",
        "    known_punct = string.punctuation\r\n",
        "    clean_text =''.join([t for t in text if not all(l in string.punctuation for l in t)])\r\n",
        "    return clean_text\r\n",
        "\r\n",
        "  #delete emoji\r\n",
        "  def deEmojify(self, text):\r\n",
        "    \"\"\"\r\n",
        "  gets string and returns the string  after Removing emojies\r\n",
        "  \r\n",
        "  :parameters: string\r\n",
        "  :returns: string  \r\n",
        "    \"\"\"  \r\n",
        "    regrex_pattern = re.compile(pattern = \"[\"\r\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\r\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\r\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\r\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\r\n",
        "                           \"]+\", flags = re.UNICODE)\r\n",
        "    return str(regrex_pattern.sub(r'', text))\r\n",
        "  \r\n",
        "  #Merging duplicates letters\r\n",
        "  def consecutive_chars_removal(self, text):\r\n",
        "    \"\"\"\r\n",
        "  Merging duplicates letters( ignoring the letter \"L\" - for allah and \"h\" for loughing)\r\n",
        "  \r\n",
        "  :parameters: text (of type string)\r\n",
        "    the text to run the function over\r\n",
        "  :returns: string  \r\n",
        "    \"\"\"\r\n",
        "    list = []\r\n",
        "    prev = \"\"\r\n",
        "    flag = True\r\n",
        "    for ch in text:\r\n",
        "        if ch != prev:\r\n",
        "            list.append(ch)\r\n",
        "        # a special case of two specifc letters in arabic\r\n",
        "        elif (flag):\r\n",
        "            if (prev == \"ل\" or prev == \"ه\"):\r\n",
        "              flag=False\r\n",
        "              list.append(ch)\r\n",
        "        if(not flag and ch != \"ل\" and ch != \"ه\"):\r\n",
        "          flag=True\r\n",
        "        prev = ch  \r\n",
        "    return ''.join(list)\r\n",
        "  \r\n",
        "  #lemmatinazer\r\n",
        "  def lemmatizer(self, text):\r\n",
        "    \"\"\"\r\n",
        "   lemmatize a text to list\r\n",
        "\r\n",
        "   module using: \"qalsadi.lemmatizer\"\r\n",
        "  \r\n",
        "  :parameters: string\r\n",
        "  \r\n",
        "  :returns: list  \r\n",
        "    \"\"\"\r\n",
        "    lemmer = qalsadi.lemmatizer.Lemmatizer()\r\n",
        "    return lemmer.lemmatize_text(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SM_E13r1WHb2"
      },
      "source": [
        "def consecutive_chars_removal( text):\r\n",
        "    \"\"\"\r\n",
        "  Merging duplicates letters( ignoring the letter \"L\" - for allah and \"h\" for loughing)\r\n",
        "  \r\n",
        "  :parameters: text (of type string)\r\n",
        "    the text to run the function over\r\n",
        "  :returns: string  \r\n",
        "    \"\"\"\r\n",
        "    list = []\r\n",
        "    prev = \"\"\r\n",
        "    flag = True\r\n",
        "    for ch in text:\r\n",
        "        if ch != prev:\r\n",
        "            list.append(ch)\r\n",
        "        # a special case of two specifc letters in arabic\r\n",
        "        elif (flag):\r\n",
        "            if (prev == \"ل\" or prev == \"ه\"):\r\n",
        "              flag=False\r\n",
        "              list.append(ch)\r\n",
        "        if(not flag and ch != \"ل\" and ch != \"ه\"):\r\n",
        "          flag=True\r\n",
        "\r\n",
        "        prev = ch  \r\n",
        "    return ''.join(list)\r\n",
        "  \r\n",
        "print(consecutive_chars_removal(\"לדחסכמה  חחחחח  نءةىىىىىالبيت الكبير  رئيس اللليابان هههههه جغففقكقق الللىءؤىو الله اكبر\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MnBbimXEOBr"
      },
      "source": [
        "preProcess_obj = preProcess(tokenizer_model='gigabert', remove_consecutive_chars=True, pure_arabic=True, punc_remove=False, remove_emoji=True, extract_emoji_sentiment=False)\r\n",
        "test_text = \")( /\\][#$!@شركة شراء اثاث مستعمل بالرياض بافضل الاسعار\"\r\n",
        "preProcess_obj.transform(test_text)\r\n",
        "print(preProcess_obj.tokenized_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lhnU-P_oVvh"
      },
      "source": [
        "preProcess_obj.tokenized_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nl0VNpRHoX9g"
      },
      "source": [
        "preProcess_obj.transform(\" اللهمّ أعلِ شأني وقدري ومكانتي في الدنيا وفي الآخرة.\")\r\n",
        "#preProcess_obj.tokenized_text\r\n",
        "preProcess_obj.lemmatized_text"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}