{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pre-process workshop.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ehudb9/preProcessNlp/blob/main/Pre_process_workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6odhHwejCOEq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a23bd530-8eba-4b47-aa84-5d6b7bc5899b"
      },
      "source": [
        "!pip install emoji \n",
        "!git clone https://github.com/aub-mind/arabert.git\n",
        "!git clone https://github.com/MagedSaeed/farasapy.git\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (0.6.0)\n",
            "fatal: destination path 'arabert' already exists and is not an empty directory.\n",
            "fatal: destination path 'farasapy' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvy2ZfWnRz9J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a70d0ce-5f8c-4e4b-bc4c-b17ba007babe"
      },
      "source": [
        "# to count words in string \n",
        "# using regex (findall()) \n",
        "#<Avrahami>: What is this code used for? why do we need it? - basic stats\n",
        "# This function will help us count number of words in a sentence.\n",
        "# Sentence with up to 5 words will be irrelevent for our algorithm.\n",
        "#why? short sentences justisying/ Deny the authors twit and don't share new info.\n",
        "import re \n",
        "\n",
        "# initializing string \n",
        "test_string = \"Ù…Ù† Ø£ÙŠÙ† Ø£Ù†Øª\"\n",
        "\n",
        "# printing original string \n",
        "print (\"The original string is : \" + test_string) \n",
        "\n",
        "# using regex (findall()) \n",
        "# to count words in string \n",
        "res = len(re.findall(r'\\w+', test_string)) \n",
        "\n",
        "# printing result \n",
        "print (\"The number of words in string are : \" + str(res)) \n",
        "\n",
        "def words_counter(text):\n",
        "  res = len(re.findall(r'\\w+', test_string))\n",
        "  return res\n",
        "\n",
        "test_string = \"Ù…Ù† Ø£ÙŠÙ† Ø£Ù†Øª\"\n",
        "words_counter(test_string)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The original string is : Ù…Ù† Ø£ÙŠÙ† Ø£Ù†Øª\n",
            "The number of words in string are : 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGGjg1V0ze0o",
        "outputId": "c608089b-e626-49d3-c39c-f4c6261519c9"
      },
      "source": [
        "# Non-Arabic characters removal- keep arabic and numeric chars and remove the rest\r\n",
        "#!/usr/bin/env python\r\n",
        "import re\r\n",
        "\r\n",
        "text =  u' Ù…Ù† Ø£ÙŠÙ† Ø£Ù†Øª  '\r\n",
        "\r\n",
        "def Remove_english(text):\r\n",
        "\r\n",
        "  t = re.sub(r'[^0-9\\u0600-\\u06ff\\u0750-\\u077f\\ufb50-\\ufbc1\\ufbd3-\\ufd3f\\ufd50-\\ufd8f\\ufd50-\\ufd8f\\ufe70-\\ufefc\\uFDF0-\\uFDFD]+', ' ', text)\r\n",
        "  return t\r\n",
        "\r\n",
        "print(Remove_english(text))\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Ù…Ù† Ø£ÙŠÙ† Ø£Ù†Øª \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcMw17WDT2yg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a405c8bb-df81-4561-f992-820d8e8d7fc5"
      },
      "source": [
        "#<Avrahami>: What is this code used for? why do we need it? - not really required\n",
        "# Emoji removal\n",
        "#!/usr/bin/env python\n",
        "import re\n",
        "\n",
        "text = u'\\U0001f602 Ù…Ù† Ø£ÙŠÙ† Ø£Ù†Øª '\n",
        "print(text) # with emoji\n",
        "\n",
        "def deEmojify(text):\n",
        "    regrex_pattern = re.compile(pattern = \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags = re.UNICODE)\n",
        "    return regrex_pattern.sub(r'',text)\n",
        "\n",
        "print(deEmojify(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ğŸ˜‚ Ù…Ù† Ø£ÙŠÙ† Ø£Ù†Øª \n",
            " Ù…Ù† Ø£ÙŠÙ† Ø£Ù†Øª \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdmickCrVO0M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4b3de57-fd6f-4373-ce27-f6fe5473b67b"
      },
      "source": [
        "#<Avrahami>: What is this code used for? why do we need it? - emoji counter\n",
        "#emoji counter + word counter \n",
        "import emoji\n",
        "import regex\n",
        "\n",
        "arabic= \"Ø­Ø³Ø¨ÙŠ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙ‡Ù…ğŸ’”\"\n",
        "line = \"hello ğŸ‘©ğŸ¾â€ğŸ“ emoji hello ğŸ‘¨â€ğŸ‘©â€ğŸ‘¦â€ğŸ‘¦ how are ğŸ˜Š you todayğŸ™…ğŸ½ğŸ™…ğŸ½\"\n",
        "counter1 = split_count(arabic)\n",
        "counter2 = split_count(line)\n",
        "print(\"Number of emojis arabic - {}, number of words - {}\".format(counter1[0], counter1[1]))\n",
        "print(\"Number of emojis line - {}, number of words - {}\".format(counter2[0], counter2[1]))\n",
        "def split_count(text):\n",
        "    emoji_counter = 0\n",
        "    data = regex.findall(r'\\X', text)\n",
        "    for word in data:\n",
        "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
        "            emoji_counter += 1\n",
        "            # Remove from the given text the emojis\n",
        "            text = text.replace(word, '') \n",
        "\n",
        "    words_counter = len(text.split())\n",
        "\n",
        "    return emoji_counter, words_counter"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of emojis arabic - 1, number of words - 3\n",
            "Number of emojis line - 5, number of words - 7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-kLBm4GYP2u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2521bcb6-f6cf-4ad2-bcf7-3ef9f2e24ec4"
      },
      "source": [
        "#<Avrahami>: \n",
        "#   (i) I have renamed the function\n",
        "#   (ii) add an input checker - make sure we indeed got a string\n",
        "#   (iii) you are currntly removing ALL chars which are consecutive. What about valid words with two (or more) consecutive chars (e.g., in English words like bool, meeting)\n",
        "#   (iv) seems like you assume all chars are arabic chars. What about punctuations? what about emojies? I would say it is better first to tokenize the text and then apply the logic\n",
        "#   (vi) add documantation - parameters explanation + what the function is doing \n",
        "#Erase duplicates in a word \n",
        "def consecutive_chars_removal(string):\n",
        "    list = []\n",
        "    prev = \"\"\n",
        "    count = 0 \n",
        "    for ch in string:\n",
        "        if ch != prev:\n",
        "            list.append(ch)\n",
        "        \"\"\"\n",
        "        # a special case of two specifc letters in arabic\n",
        "        elif ch == \"Ù„\" or ch == \"Ù‡\":\n",
        "          if count < 2 :\n",
        "             count +=1\n",
        "             list.append(ch)\n",
        "          else:\n",
        "            count = 0 \n",
        "        \"\"\"\n",
        "        prev = ch  \n",
        "    return ''.join(list)        \n",
        "\n",
        "string = \"Ø´ÙˆÙØªÙƒÙƒÙƒÙƒÙƒÙƒÙƒÙƒÙƒÙƒÙƒÙƒÙƒÙƒÙƒÙƒÙƒÙƒÙƒÙƒÙƒÙƒ\"\n",
        "string1=\"ØªÙ‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù†\"\n",
        "string2=\"Ø¹Ø¸ÙŠÙŠÙŠÙŠÙŠÙŠÙ…Ù…Ù…Ù…Ù…Ù…Ù…\"\n",
        "string3=\"Ù„Ø§Ø§Ù†Ù„Ù†ØªØªÙ‰Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„ Ø§Ù„Ø© Ø§Ù„Ø§ Ø§Ù„Ù„Ù‚\"\n",
        "print(fix(string))\n",
        "print(fix(string1))\n",
        "print(fix(string2))\n",
        "print(fix(string3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ø´ÙˆÙØªÙƒ\n",
            "ØªÙ‡Ù†\n",
            "Ø¹Ø¸ÙŠÙ…\n",
            "Ù„Ø§Ù†Ù„Ù†ØªÙ‰Ù„ Ø§Ù„Ø© Ø§Ù„Ø§ Ø§Ù„Ù‚\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xi8ix3ibZTFE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27bad56d-74a6-4bd1-fa98-ff93748c455f"
      },
      "source": [
        "#<Avrahami>: What is this code used for? why do we need it?\n",
        "#Emoji translate\n",
        "from emoji.unicode_codes import UNICODE_EMOJI\n",
        "\n",
        "emoji = 'ğŸ˜Š'\n",
        "\n",
        "print(UNICODE_EMOJI[emoji])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ":smiling_face_with_smiling_eyes:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOFZJy0lkscS"
      },
      "source": [
        "\n",
        "#<Avrahami>: What is this code used for? why do we need it?\n",
        "#https://pypi.org/project/emosent-py/\n",
        "#https://github.com/FintelLabs/emosent-py/tree/master/emosent\n",
        "#http://kt.ijs.si/data/Emoji_sentiment_ranking/\n",
        "#http://kt.ijs.si/data/Emoji_sentiment_ranking/emojimap.html\n",
        "!pip install emosent-py\n",
        "from emosent import get_emoji_sentiment_rank\n",
        "get_emoji_sentiment_rank('ğŸ’”')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2LeACkGmq4b"
      },
      "source": [
        "!pip install emosent-py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kD7i-YPP9kXX"
      },
      "source": [
        "!pip install git+https://github.com/aub-mind/arabert.git\n",
        "from cltk.corpus.utils.importer import CorpusImporter\n",
        " c = CorpusImporter('arabic')\n",
        " c.list_corpora\n",
        " c.import_corpus('arabic_text_perseus') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whe0XNwrxtcA"
      },
      "source": [
        "!pip install git+https://github.com/CAMeL-Lab/camel_tools.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFj6bl-vnoXX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6_x01CfsA9h"
      },
      "source": [
        "tokenizer_obj = arabic_tokenizer(model_name='arabert')\n",
        "text_to_tokenize = 'Ù‚ÙˆØ§Ù†ÙŠÙ†Ù‡Ø§ Ùˆ Ù…Ø¯Ø© Ø§Ù„Ù„Ø¹Ø¨ Ø§Ø·ÙˆÙ„ Ù…Ù† Ø­ÙŠØ§ØªÙŠ ÙˆÙƒØ£Ù†Ù‡Ø§ Ù„Ø¹Ø¨Ø© Ù…Ù† Ø§Ù„Ø¹ØµØ± Ø§Ù„Ù‡ÙŠØ±ÙˆØºÙ„ÙŠÙÙŠØŒ Ù…Ø§ ÙÙ‡ØªÙ…ØªÙ‡Ø§ Ùˆ Ø¨Ø¹Ø±ÙØ´ ÙƒÙŠÙ Ø±Ø¨Ø­Øª Ù„Ù…Ø§ Ù„Ø¹Ø¨ØªÙ‡Ø§ Ù…Ø±Ø© Ø¨Ø³.'\n",
        "tokenizer_obj.tokenize(text_to_tokenize, remove_punckt=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOwvjC5-pueM",
        "outputId": "e1e06e7c-a1e2-457e-e769-b9903f8801bd"
      },
      "source": [
        "import string\r\n",
        "known_punct = string.punctuation\r\n",
        "my_tokenized_data = ['asa!!~@#', '##', 'aaa']\r\n",
        "\r\n",
        "tokenized_text_filtered = [t for t in my_tokenized_data if not all(l in string.punctuation for l in t)]\r\n",
        "tokenized_text_filtered"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['asa!!~@#', 'aaa']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oBMnjhHaaaF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JG06pXfmZ66W"
      },
      "source": [
        "##**lematizetion**\r\n",
        " list for checking:\r\n",
        "\r\n",
        "\r\n",
        "1.   X:farasa - --need to find api\r\n",
        "2.   qalsadi \r\n",
        "3.  X: ALKHALIL ---need to find api\r\n",
        "4. nltk ==> returns the root. us ut good?\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-jDM_qbbEMc"
      },
      "source": [
        "!pip install qalsadi\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8MxJWaMaUaK"
      },
      "source": [
        "# lematization --- returns the root ---> to check with avrahami\r\n",
        "\r\n",
        "\r\n",
        "from nltk.stem.isri import ISRIStemmer\r\n",
        "st = ISRIStemmer()\r\n",
        "print(st.stem(u''))\r\n",
        "\r\n",
        "#print(ISRIStemmer().suf32(\"Ø§Ø¹Ù„Ø§Ù…ÙŠÙˆÙ†\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fbzwMB-d0ef"
      },
      "source": [
        "# using qalsadi  --> more options in https://pypi.org/project/qalsadi/  + analysing\r\n",
        "\r\n",
        "import qalsadi.lemmatizer\r\n",
        "text = u\"\"\"Ù‡Ù„ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ±Ø¬Ù…Ø© ÙƒÙŠ ØªÙÙ‡Ù… Ø®Ø·Ø§Ø¨ Ø§Ù„Ù…Ù„ÙƒØŸ Ø§Ù„Ù„ØºØ© \"Ø§Ù„ÙƒÙ„Ø§Ø³ÙŠÙƒÙŠØ©\" (Ø§Ù„ÙØµØ­Ù‰) Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ ÙƒÙ„ Ø§Ù„Ù„ØºØ§Øª ÙˆÙƒØ°Ù„Ùƒ Ø§Ù„Ù„ØºØ© \"Ø§Ù„Ø¯Ø§Ø±Ø¬Ø©\" .. Ø§Ù„ÙØ±Ù†Ø³ÙŠØ© Ø§Ù„ØªÙŠ Ù†Ø¯Ø±Ø³ ÙÙŠ Ø§Ù„Ù…Ø¯Ø±Ø³Ø© Ù„ÙŠØ³Øª Ø§Ù„ÙØ±Ù†Ø³ÙŠØ© Ø§Ù„ØªÙŠ ÙŠØ³ØªØ®Ø¯Ù…Ù‡Ø§ Ø§Ù„Ù†Ø§Ø³ ÙÙŠ Ø´ÙˆØ§Ø±Ø¹ Ø¨Ø§Ø±ÙŠØ³ .. ÙˆÙ…Ù„ÙƒØ© Ø¨Ø±ÙŠØ·Ø§Ù†ÙŠØ§ Ù„Ø§ ØªØ®Ø·Ø¨ Ø¨Ù„ØºØ© Ø´ÙˆØ§Ø±Ø¹ Ù„Ù†Ø¯Ù† .. Ù„ÙƒÙ„ Ù…Ù‚Ø§Ù… Ù…Ù‚Ø§Ù„\"\"\"\r\n",
        "lemmer = qalsadi.lemmatizer.Lemmatizer()\r\n",
        "# lemmatize a word\r\n",
        "print(lemmer.lemmatize(\"ÙŠØ­ØªØ§Ø¬\"))\r\n",
        "\r\n",
        "# lemmatize a word with a specific pos\r\n",
        "print(lemmer.lemmatize(\"ÙˆÙÙŠ\"))\r\n",
        "lemmer.lemmatize(\"ÙˆÙÙŠ\", pos=\"v\")\r\n",
        "\r\n",
        "# lemmatize a text to list\r\n",
        "lemmas = lemmer.lemmatize_text(text)\r\n",
        "print(lemmas)\r\n",
        "\r\n",
        "# lemmatize a text and return lemma pos\r\n",
        "lemmas = lemmer.lemmatize_text(text, return_pos=True)\r\n",
        "print(lemmas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnZCFJ_xplvw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rz2Rr1GJplyz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vfNIV-ooyn5"
      },
      "source": [
        "# **pre-process class**\r\n",
        "last update: 21.12.2020, Avrahami"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hL9vxsW0o5Ww"
      },
      "source": [
        "#installations and imports\r\n",
        "!pip install qalsadi\r\n",
        "! pip install transformers\r\n",
        "! pip install pyarabic\r\n",
        "!pip install emoji\r\n",
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "\r\n",
        "from transformers import AutoTokenizer, BertTokenizer\r\n",
        "import pyarabic.araby as araby\r\n",
        "from emoji.unicode_codes import UNICODE_EMOJI\r\n",
        "import string\r\n",
        "\r\n",
        "import re \r\n",
        "import qalsadi.lemmatizer\r\n",
        "from nltk.stem.isri import ISRIStemmer\r\n",
        "from emoji.unicode_codes import UNICODE_EMOJI\r\n",
        "#from emosent import get_emoji_sentiment_rank\r\n",
        "import regex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rarmuzonogt"
      },
      "source": [
        "class arabic_tokenizer(object):\n",
        "    \"\"\" Class for tokenize arabic text in different methods\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "\n",
        "        Attributes\n",
        "        ----------\n",
        "\n",
        "        Examples\n",
        "        --------\n",
        "\n",
        "        \"\"\"\n",
        "    def __init__(self, model_name):\n",
        "      self.model_name = model_name\n",
        "      if self.model_name == 'arabert':\n",
        "        model_str = 'aubmindlab/bert-base-arabert'\n",
        "        arabert_tokenizer = AutoTokenizer.from_pretrained(model_str)\n",
        "        arabert_tokenizer = self.add_emojies_to_tokenizer(arabert_tokenizer)\n",
        "        self.tokenizer = arabert_tokenizer.tokenize\n",
        "      elif self.model_name == 'gigabert':\n",
        "        model_str = 'lanwuwei/GigaBERT-v4-Arabic-and-English'\n",
        "        gigabert_tokenizer = BertTokenizer.from_pretrained(model_str, do_lower_case=True)\n",
        "        gigabert_tokenizer = self.add_emojies_to_tokenizer(gigabert_tokenizer)\n",
        "        self.tokenizer = gigabert_tokenizer.tokenize\n",
        "      elif self.model_name == 'nltk':\n",
        "        self.tokenizer = nltk.word_tokenize\n",
        "      elif self.model_name == 'pyarabic':\n",
        "        self.tokenizer = araby.tokenize\n",
        "    def get_model(self):\n",
        "      return self.model\n",
        "\n",
        "    def set_model(self, model_name):\n",
        "      self.model = model_name\n",
        "      \n",
        "    def tokenize(self, text, remove_punckt=True):\n",
        "      tokenized_text = self.tokenizer(text)\n",
        "      if remove_punckt:\n",
        "        known_punct = string.punctuation\n",
        "        tokenized_text_filtered = [t for t in tokenized_text if not all(l in string.punctuation for l in t)]\n",
        "        return tokenized_text_filtered\n",
        "      else:\n",
        "        return tokenized_text\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def add_emojies_to_tokenizer(tokenizer):\n",
        "      for cur_emoji in UNICODE_EMOJI.keys():\n",
        "        tokenizer.add_tokens(cur_emoji)\n",
        "      return tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dgqgh9-CiBF1"
      },
      "source": [
        "class preProcess:\r\n",
        "  \"\"\" Class for pre-processing text for analyzing\r\n",
        "      gets a text and creating a preProcess object with the following Attributes:\r\n",
        "      1. text :  the input text\r\n",
        "      2. Emojie : the text with no emojies , emojies sentiment\r\n",
        "      3. text satistic: word counts , sentences\r\n",
        "      4. cleaner : the text after Removing punctuation, stop words and emojies, and Merging duplicates ( ignoring the letter \"L\" - for allah) \r\n",
        "      5.Tokenize : list of the words after cleaner and tokenization \r\n",
        "      6. Lemmatization : list of the words after cleaner and tokenization and Lemmatization\r\n",
        "\r\n",
        "      pipline of the process : text input(string) :\r\n",
        "      *Emoji sentiment and remove emojies\r\n",
        "      *text satistic:word counts,sentences\r\n",
        "      *cleaner: \r\n",
        "          1. Removing punctuation\r\n",
        "          2. Removing stop words and emojies\r\n",
        "          3. Merging duplicates ( ignoring the letter \"L\" - for allah)\r\n",
        "      *Tokenizer the text\r\n",
        "      *Lemmatization\r\n",
        "        Parameters\r\n",
        "        :text: string for processing\r\n",
        "\r\n",
        "        Examples\r\n",
        "        text =\"Ø´Ø±ÙƒØ© Ø´Ø±Ø§Ø¡ Ø§Ø«Ø§Ø« Ù…Ø³ØªØ¹Ù…Ù„ Ø¨Ø§Ù„Ø±ÙŠØ§Ø¶ Ø¨Ø§ÙØ¶Ù„ Ø§Ù„Ø§Ø³Ø¹Ø§Ø± \"\r\n",
        "        print (test.lemmatization) --> ['Ø´Ø±ÙƒØ©', 'Ø´Ø±Ø§Ø¡', 'Ø§Ø«Ø§Ø«', 'Ù…Ø³ØªØ¹Ù…Ù„', 'Ø±ÙŠØ§Ø¶', 'Ø¨Ø§ÙØ¶Ù„', 'Ø§Ù„Ø§Ø³Ø¹Ø§Ø±']\r\n",
        "\r\n",
        "        \"\"\"\r\n",
        "  def __init__(self, tokenizer_model='gigabert', remove_consecutive_chars=True, pure_arabic=True,punc_remove=True,remove_emoji=True, extract_emoji_sentiment=False):\r\n",
        "    \"\"\"   \r\n",
        "    creating a preProcess object with the following Attributes:\r\n",
        "      1. text :  the input text\r\n",
        "      2. Emojie : the text with no emojies , emojies sentiment\r\n",
        "      3. text satistic: word counts , sentences\r\n",
        "      4. cleaner : text after Removing punctuation, stop words and emojies, and Merging duplicates ( ignoring the letter \"L\" - for allah) \r\n",
        "      5. Tokenizer : list of the words after cleaner and tokenization \r\n",
        "      6. Lemmatization : list of the words after cleaner and tokenization and Lemmatization\r\n",
        "  \r\n",
        "    Parameters: \r\n",
        "    :text (string): string for processing\r\n",
        "   \r\n",
        "    \"\"\"\r\n",
        "    self.remove_consecutive_chars = remove_consecutive_chars\r\n",
        "    self.punc_remove = punc_remove\r\n",
        "    self.extract_emoji_sentiment = extract_emoji_sentiment\r\n",
        "    self.remove_emoji = remove_emoji\r\n",
        "    self.pure_arabic = pure_arabic\r\n",
        "    self.tokenizer = arabic_tokenizer(model_name=tokenizer_model)\r\n",
        "    self.satistics = None\r\n",
        "    self.emojies_sentiment = None\r\n",
        "    self.tokenized_text = None\r\n",
        "    self.lemmatized_text = None\r\n",
        "     \r\n",
        "  def transform(self, text):\r\n",
        "    #checking if the text is string  \r\n",
        "    if (not isinstance(text, str)) : \r\n",
        "      raise Exception(\"The input is not a string\")\r\n",
        "    new_text = text\r\n",
        "    if self.pure_arabic:\r\n",
        "      new_text = self.remove_non_arabic_char(new_text)\r\n",
        "    if self.remove_consecutive_chars:\r\n",
        "      new_text = self.consecutive_chars_removal(new_text)\r\n",
        "    if self.punc_remove:\r\n",
        "      new_text = self.punc_removal(new_text)\r\n",
        "    if self.extract_emoji_sentiment:\r\n",
        "      self.emojies_sentiment =  self.emoji_sentiment()    #to implemete tne sentimente\r\n",
        "    if self.remove_emoji:\r\n",
        "      new_text = self.deEmojify(new_text)\r\n",
        "    \r\n",
        "    self.satistics = self.words_counter(new_text) # int: num of words\r\n",
        "    self.tokenized_text = self.tokenizer.tokenize(new_text, remove_punckt=self.punc_remove)\r\n",
        "    self.lemmatized_text = self.lemmatizer(new_text)\r\n",
        "    return new_text\r\n",
        "  \r\n",
        "  \r\n",
        "  \"\"\" to add more satistic parameters\r\n",
        "  def calc_stats(text):\r\n",
        "    #checking if the text is string  \r\n",
        "    if (not isinstance(text, str)) : \r\n",
        "      raise Exception(\"The input is not a string\")\r\n",
        "    self.satistics = {\"word_count\" : self.words_counter() } # te comlete sentence counter\r\n",
        "    return self.satistics\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  # emoji func\r\n",
        "  def emoji_sentiment(self):\r\n",
        "    # lack of implementation!!! neet to check if needed with avrahami and if to do it before or after cleanning\r\n",
        "    return \"in process\"\r\n",
        "  \r\n",
        "  # satistic func\r\n",
        "  def words_counter(self, text):\r\n",
        "    \"\"\"\r\n",
        "    gets string and returns th number of the word (int) \r\n",
        "  \r\n",
        "    \"\"\"  \r\n",
        "    return len(re.findall(r'\\w+', text))\r\n",
        "  \r\n",
        "  # Non-Arabic characters removal- keep arabic and numeric chars and remove the rest\r\n",
        "  def remove_non_arabic_char(self, text):\r\n",
        "  # Gets String and remove non arabic letters\r\n",
        "    t = re.sub(r'[^0-9\\u0600-\\u06ff\\u0750-\\u077f\\ufb50-\\ufbc1\\ufbd3-\\ufd3f\\ufd50-\\ufd8f\\ufd50-\\ufd8f\\ufe70-\\ufefc\\uFDF0-\\uFDFD-\\~@#$^*()_+=[\\]{}|\\\\,.?: -@#$% <>` \"]+', ' ', text)\r\n",
        "    return t\r\n",
        "\r\n",
        "\r\n",
        "  #cleaner\r\n",
        "  def punc_removal(self, text):\r\n",
        "    \"\"\"\r\n",
        "  gets string and returns the string  after Removing punctuation, stop words and emojies,\r\n",
        "  and Merging duplicates ( ignoring the letter \"L\" - for allah)\r\n",
        "  \r\n",
        "    \"\"\"  \r\n",
        "    known_punct = string.punctuation\r\n",
        "    clean_text =''.join([t for t in text if not all(l in string.punctuation for l in t)])\r\n",
        "    return clean_text\r\n",
        "\r\n",
        "  #delete emoji\r\n",
        "  def deEmojify(self, text):\r\n",
        "    \"\"\"\r\n",
        "  gets string and returns the string  after Removing emojies\r\n",
        "  \r\n",
        "  :parameters: string\r\n",
        "  :returns: string  \r\n",
        "    \"\"\"  \r\n",
        "    regrex_pattern = re.compile(pattern = \"[\"\r\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\r\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\r\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\r\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\r\n",
        "                           \"]+\", flags = re.UNICODE)\r\n",
        "    return str(regrex_pattern.sub(r'', text))\r\n",
        "  \r\n",
        "  #Merging duplicates letters\r\n",
        "  def consecutive_chars_removal(self, text):\r\n",
        "    \"\"\"\r\n",
        "  Merging duplicates letters( ignoring the letter \"L\" - for allah and \"h\" for loughing)\r\n",
        "  \r\n",
        "  :parameters: text (of type string)\r\n",
        "    the text to run the function over\r\n",
        "  :returns: string  \r\n",
        "    \"\"\"\r\n",
        "    list = []\r\n",
        "    prev = \"\"\r\n",
        "    flag = True\r\n",
        "    for ch in text:\r\n",
        "        if ch != prev:\r\n",
        "            list.append(ch)\r\n",
        "        # a special case of two specifc letters in arabic\r\n",
        "        elif (flag):\r\n",
        "            if (prev == \"Ù„\" or prev == \"Ù‡\"):\r\n",
        "              flag=False\r\n",
        "              list.append(ch)\r\n",
        "        if(not flag and ch != \"Ù„\" and ch != \"Ù‡\"):\r\n",
        "          flag=True\r\n",
        "        prev = ch  \r\n",
        "    return ''.join(list)\r\n",
        "  \r\n",
        "  #lemmatinazer\r\n",
        "  def lemmatizer(self, text):\r\n",
        "    \"\"\"\r\n",
        "   lemmatize a text to list\r\n",
        "\r\n",
        "   module using: \"qalsadi.lemmatizer\"\r\n",
        "  \r\n",
        "  :parameters: string\r\n",
        "  \r\n",
        "  :returns: list  \r\n",
        "    \"\"\"\r\n",
        "    lemmer = qalsadi.lemmatizer.Lemmatizer()\r\n",
        "    return lemmer.lemmatize_text(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SM_E13r1WHb2"
      },
      "source": [
        "def consecutive_chars_removal( text):\r\n",
        "    \"\"\"\r\n",
        "  Merging duplicates letters( ignoring the letter \"L\" - for allah and \"h\" for loughing)\r\n",
        "  \r\n",
        "  :parameters: text (of type string)\r\n",
        "    the text to run the function over\r\n",
        "  :returns: string  \r\n",
        "    \"\"\"\r\n",
        "    list = []\r\n",
        "    prev = \"\"\r\n",
        "    flag = True\r\n",
        "    for ch in text:\r\n",
        "        if ch != prev:\r\n",
        "            list.append(ch)\r\n",
        "        # a special case of two specifc letters in arabic\r\n",
        "        elif (flag):\r\n",
        "            if (prev == \"Ù„\" or prev == \"Ù‡\"):\r\n",
        "              flag=False\r\n",
        "              list.append(ch)\r\n",
        "        if(not flag and ch != \"Ù„\" and ch != \"Ù‡\"):\r\n",
        "          flag=True\r\n",
        "\r\n",
        "        prev = ch  \r\n",
        "    return ''.join(list)\r\n",
        "  \r\n",
        "print(consecutive_chars_removal(\"×œ×“×—×¡×›××”  ×—×—×—×—×—  Ù†Ø¡Ø©Ù‰Ù‰Ù‰Ù‰Ù‰Ø§Ù„Ø¨ÙŠØª Ø§Ù„ÙƒØ¨ÙŠØ±  Ø±Ø¦ÙŠØ³ Ø§Ù„Ù„Ù„ÙŠØ§Ø¨Ø§Ù† Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡ Ø¬ØºÙÙÙ‚ÙƒÙ‚Ù‚ Ø§Ù„Ù„Ù„Ù‰Ø¡Ø¤Ù‰Ùˆ Ø§Ù„Ù„Ù‡ Ø§ÙƒØ¨Ø±\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MnBbimXEOBr"
      },
      "source": [
        "preProcess_obj = preProcess(tokenizer_model='gigabert', remove_consecutive_chars=True, pure_arabic=True, punc_remove=False, remove_emoji=True, extract_emoji_sentiment=False)\r\n",
        "test_text = \")( /\\][#$!@Ø´Ø±ÙƒØ© Ø´Ø±Ø§Ø¡ Ø§Ø«Ø§Ø« Ù…Ø³ØªØ¹Ù…Ù„ Ø¨Ø§Ù„Ø±ÙŠØ§Ø¶ Ø¨Ø§ÙØ¶Ù„ Ø§Ù„Ø§Ø³Ø¹Ø§Ø±\"\r\n",
        "preProcess_obj.transform(test_text)\r\n",
        "print(preProcess_obj.tokenized_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lhnU-P_oVvh"
      },
      "source": [
        "preProcess_obj.tokenized_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nl0VNpRHoX9g"
      },
      "source": [
        "preProcess_obj.transform(\" Ø§Ù„Ù„Ù‡Ù…Ù‘ Ø£Ø¹Ù„Ù Ø´Ø£Ù†ÙŠ ÙˆÙ‚Ø¯Ø±ÙŠ ÙˆÙ…ÙƒØ§Ù†ØªÙŠ ÙÙŠ Ø§Ù„Ø¯Ù†ÙŠØ§ ÙˆÙÙŠ Ø§Ù„Ø¢Ø®Ø±Ø©.\")\r\n",
        "#preProcess_obj.tokenized_text\r\n",
        "preProcess_obj.lemmatized_text"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}